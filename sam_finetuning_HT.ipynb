{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eba6eb8",
   "metadata": {},
   "source": [
    "# Finetuning Segment Anything with µsam\n",
    "\n",
    "This notebook shows how to use Segment Anything for Microscopy to fine-tune a Segment Anything Model (SAM) on your custom data.\n",
    "\n",
    "We use DIC microscopy images from the Cell Tracking Challenge, DIC-C2DH-HeLa, HeLa cells on a flat glass (from [Maška et al.](https://www.nature.com/articles/s41592-023-01879-y)) in this notebook. The functionalities shown here should work for your (microscopy) images too.\n",
    "\n",
    "You can find the latest version of this notebook at https://github.com/computational-cell-analytics/micro-sam/blob/master/notebooks/sam_finetuning.ipynb. We are still improving the notebook to explain the individual steps better. If you come back to this exercise later please check the original notebook for updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84f98e8",
   "metadata": {},
   "source": [
    "### Importing the libraries\n",
    "\n",
    "We import all libraries needed for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9024ce53",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from IPython.display import FileLink\n",
    "\n",
    "import numpy as np\n",
    "import imageio.v3 as imageio\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch_em\n",
    "from torch_em.model import UNETR\n",
    "from torch_em.util.debug import check_loader\n",
    "from torch_em.loss import DiceBasedDistanceLoss\n",
    "from torch_em.transform.label import PerObjectDistanceTransform\n",
    "\n",
    "from micro_sam import util\n",
    "import micro_sam.training as sam_training\n",
    "from micro_sam.sample_data import fetch_tracking_example_data, fetch_tracking_segmentation_data\n",
    "from micro_sam.instance_segmentation import (\n",
    "    InstanceSegmentationWithDecoder,\n",
    "    get_predictor_and_decoder,\n",
    "    mask_data_to_segmentation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18166aa6",
   "metadata": {},
   "source": [
    "### Let's download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956c4467",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = os.path.join(root_dir, \"data\")\n",
    "os.makedirs(DATA_FOLDER, exist_ok=True)\n",
    "\n",
    "# This will download the image and segmentation data for training.\n",
    "image_dir = fetch_tracking_example_data(DATA_FOLDER)\n",
    "segmentation_dir = fetch_tracking_segmentation_data(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89489478",
   "metadata": {},
   "source": [
    "### Let's create the dataloaders\n",
    "\n",
    "Our task is to segment HeLa cells on a flat glass in DIC microscopic images. The dataset comes from https://celltrackingchallenge.net/2d-datasets/, and the dataloader has been implemented in [torch-em](https://github.com/constantinpape/torch-em/blob/main/torch_em/data/datasets/ctc.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22344ae",
   "metadata": {},
   "source": [
    "#### First, let's visualize how our samples look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484a790a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "image_paths = sorted(glob(os.path.join(image_dir, \"*\")))\n",
    "segmentation_paths = sorted(glob(os.path.join(segmentation_dir, \"*\")))\n",
    "\n",
    "for image_path, segmentation_path in zip(image_paths, segmentation_paths):\n",
    "    image = imageio.imread(image_path)\n",
    "    segmentation = imageio.imread(segmentation_path)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "    ax[0].imshow(image, cmap=\"gray\")\n",
    "    ax[0].set_title(\"Input Image\")\n",
    "    ax[0].axis(\"off\")\n",
    "    \n",
    "    ax[1].imshow(segmentation)\n",
    "    ax[1].set_title(\"Ground Truth Instances\")\n",
    "    ax[1].axis(\"off\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    break  # comment this out in case you want to visualize all the images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cdcb97",
   "metadata": {},
   "source": [
    "#### Next, let's create the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9b8aff",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# torch_em.default_segmentation_loader is a convenience function to build a torch dataloader\n",
    "# from image data and labels for training segmentation models.\n",
    "# It supports image data in various formats. Here, we load image data and labels from the two\n",
    "# folders with tif images that were downloaded by the example data functionality, by specifying\n",
    "# `raw_key` and `label_key` as `*.tif`. This means all images in the respective folders that end with\n",
    "# .tif will be loadded.\n",
    "# The function supports many other file formats. For example, if you have tif stacks with multiple slices\n",
    "# instead of multiple tif images in a foldder, then you can pass raw_key=label_key=None.\n",
    "# For more information, here is the documentation: https://github.com/constantinpape/torch-em/blob/main/torch_em/data/datasets/README.md\n",
    "\n",
    "# Load images from multiple files in folder via pattern (here: all tif files)\n",
    "raw_key, label_key = \"*.tif\", \"*.tif\"\n",
    "\n",
    "# Alternative: if you have tif stacks you can just set raw_key and label_key to None\n",
    "# raw_key, label_key= None, None\n",
    "\n",
    "# The 'roi' argument can be used to subselect parts of the data.\n",
    "# Here, we use it to select the first 70 images (frames) for the train split and the other frames for the val split.\n",
    "train_roi = np.s_[:70, :, :]\n",
    "val_roi = np.s_[70:, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7935363",
   "metadata": {},
   "source": [
    "#### Label Transform\n",
    "\n",
    "The idea here is to convert the ground-truth to the desired instance for finetuning Segment Anything, and in addition if desired, to learn the foreground and distances to the object centers and object boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1e0612",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class LabelTransform:\n",
    "    def __init__(self, train_instance_segmentation):\n",
    "        self.train_instance_segmentation = train_instance_segmentation\n",
    "        \n",
    "    def __call__(self, labels):\n",
    "        if self.train_instance_segmentation:\n",
    "            # Computes the distance transform for objects to jointly perform the additional decoder-based automatic instance segmentation (AIS) and finetune Segment Anything.\n",
    "            label_transform = PerObjectDistanceTransform(\n",
    "                distances=True,\n",
    "                boundary_distances=True,\n",
    "                directed_distances=False,\n",
    "                foreground=True,\n",
    "                instances=True,\n",
    "                min_size=25\n",
    "            )\n",
    "        else:\n",
    "            # Ensures the individual object instances.to finetune the clasiscal Segment Anything.\n",
    "            label_transform = torch_em.transform.label.connected_components\n",
    "\n",
    "        labels = label_transform(labels)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16621043",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# The script below returns the train or val data loader for finetuning SAM.\n",
    "\n",
    "# The data loader must be a torch data loader that returns `x, y` tensors,\n",
    "# where `x` is the image data and `y` are the labels.\n",
    "# The labels have to be in a label mask instance segmentation format.\n",
    "# i.e. a tensor of the same spatial shape as `x`, with each object mask having its own ID.\n",
    "# Important: the ID 0 is reseved for background, and the IDs must be consecutive\n",
    "\n",
    "# Here, we use `torch_em.default_segmentation_loader` for creating a suitable data loader from\n",
    "# the example hela data. You can either adapt this for your own data or write a suitable torch dataloader yourself.\n",
    "# Here's a quickstart notebook to create your own dataloaders: https://github.com/constantinpape/torch-em/blob/main/notebooks/tutorial_create_dataloaders.ipynb\n",
    "\n",
    "batch_size = 1  # the training batch size\n",
    "patch_shape = (1, 512, 512)  # the size of patches for training\n",
    "\n",
    "# Train an additional convolutional decoder for end-to-end automatic instance segmentation\n",
    "train_instance_segmentation = True\n",
    "\n",
    "label_transform = LabelTransform(train_instance_segmentation)\n",
    "\n",
    "train_loader = torch_em.default_segmentation_loader(\n",
    "    raw_paths=image_dir,\n",
    "    raw_key=raw_key,\n",
    "    label_paths=segmentation_dir,\n",
    "    label_key=label_key,\n",
    "    patch_shape=patch_shape,\n",
    "    batch_size=batch_size,\n",
    "    ndim=2,\n",
    "    is_seg_dataset=True,\n",
    "    rois=train_roi,\n",
    "    label_transform=label_transform,\n",
    "    shuffle=True,\n",
    "    raw_transform=sam_training.identity,\n",
    ")\n",
    "val_loader = torch_em.default_segmentation_loader(\n",
    "    raw_paths=image_dir,\n",
    "    raw_key=raw_key,\n",
    "    label_paths=segmentation_dir,\n",
    "    label_key=label_key,\n",
    "    patch_shape=patch_shape,\n",
    "    batch_size=batch_size,\n",
    "    ndim=2,\n",
    "    is_seg_dataset=True,\n",
    "    rois=val_roi,\n",
    "    label_transform=label_transform,\n",
    "    shuffle=True,\n",
    "    raw_transform=sam_training.identity,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a65d8a5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's check how our samples look from the dataloader\n",
    "check_loader(train_loader, 4, plt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385b0220",
   "metadata": {},
   "source": [
    "### Run the actual model finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5962ca4b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# All hyperparameters for training.\n",
    "n_objects_per_batch = 5  # the number of objects per batch that will be sampled\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # the device/GPU used for training\n",
    "n_epochs = 10  # how long we train (in epochs)\n",
    "\n",
    "# The model_type determines which base model is used to initialize the weights that are finetuned.\n",
    "# We use vit_b here because it can be trained faster. Note that vit_h usually yields higher quality results.\n",
    "model_type = \"vit_b\"\n",
    "\n",
    "# The name of the checkpoint. The checkpoints will be stored in './checkpoints/<checkpoint_name>'\n",
    "checkpoint_name = \"sam_hela\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1910377",
   "metadata": {},
   "source": [
    "**NOTE**: You need to decide whether to finetune the Segment Anything model, or the `µsam`'s \"microscopy models\" for your dataset. Here, we finetune on the Segment Anything model for simplicity. For example, if you choose to finetune the model from the light microscopy generalist models, you need to update the `model_type` to `vit_b_lm` and it takes care of initializing the model with the desired weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade4e92",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Run training\n",
    "sam_training.train_sam(\n",
    "    name=checkpoint_name,\n",
    "    save_root=os.path.join(root_dir, \"models\"),\n",
    "    model_type=model_type,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    n_epochs=n_epochs,\n",
    "    n_objects_per_batch=n_objects_per_batch,\n",
    "    with_segmentation_decoder=train_instance_segmentation,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e472fc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's spot our best checkpoint and download it to get started with the annotation tool\n",
    "best_checkpoint = os.path.join(\"models\", \"checkpoints\", checkpoint_name, \"best.pt\")\n",
    "\n",
    "# # Download link is automatically generated for the best model.\n",
    "print(\"Click here\\u2193\")\n",
    "FileLink(best_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff79f0e",
   "metadata": {},
   "source": [
    "### Let's run the automatic instance segmentation (AIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2335ceda",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def run_automatic_instance_segmentation(image):\n",
    "    predictor, decoder = get_predictor_and_decoder(model_type=model_type, checkpoint_path=best_checkpoint, device=device)\n",
    "    image_embeddings = util.precompute_image_embeddings(predictor=predictor, input_=image, ndim=2)\n",
    "\n",
    "    ais = InstanceSegmentationWithDecoder(predictor, decoder)\n",
    "    ais.initialize(image, image_embeddings=image_embeddings)\n",
    "\n",
    "    prediction = ais.generate()\n",
    "    prediction = mask_data_to_segmentation(prediction, with_background=True)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d3c6ce",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "zip_path = os.path.join(root_dir, \"data\", \"DIC-C2DH-HeLa-test.zip\")\n",
    "!wget http://data.celltrackingchallenge.net/test-datasets/DIC-C2DH-HeLa.zip -O $zip_path\n",
    "    \n",
    "trg_dir = os.path.join(root_dir, \"data\", \"test\")\n",
    "os.makedirs(trg_dir, exist_ok=True)\n",
    "!unzip $zip_path -d trg_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0870360",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "assert os.path.exists(best_checkpoint), \"Please train the model first to run inference on the finetuned model.\"\n",
    "assert train_instance_segmentation is True, \"Oops. You didn't opt for finetuning using the decoder-based automatic instance segmentation.\"\n",
    "\n",
    "# # Let's check the first 5 images. Feel free to comment out the line below to run inference on all images.\n",
    "image_paths = image_paths[:5]\n",
    "\n",
    "for image_path in image_paths:\n",
    "    image = imageio.imread(image_path)\n",
    "    \n",
    "    # Predicted instances\n",
    "    prediction = run_automatic_instance_segmentation(image)\n",
    "\n",
    "    # Visualize the predictions\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "\n",
    "    ax[0].imshow(image, cmap=\"gray\")\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[0].set_title(\"Input Image\")\n",
    "\n",
    "    ax[1].imshow(prediction)\n",
    "    ax[1].axis(\"off\")\n",
    "    ax[1].set_title(\"Predictions (AIS)\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b4ff5",
   "metadata": {},
   "source": [
    "### What next?\n",
    "\n",
    "It's time to get started with your custom finetuned model using the annotator tool. Here is the documentation on how to get started with `µsam`: https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#annotation-tools\n",
    "\n",
    "Happy annotating!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8616ad9",
   "metadata": {},
   "source": [
    "*This notebook was last ran on Apr 22, 2024*"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
